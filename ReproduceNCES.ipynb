{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "behind-statement",
   "metadata": {},
   "source": [
    "## Evaluate NCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "written-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#def split_train_test(kb):\n",
    "#    with open(f'Method/Datasets/{kb}/Data/Data.json') as file:\n",
    "#        data = json.load(file)\n",
    "#    data = list(data.items())\n",
    "#    data_train, data_test = train_test_split(data, test_size=0.01, random_state=42)\n",
    "#    os.makedirs(f'Method/Datasets/{kb}/Test_data/', exist_ok=True)\n",
    "#    with open(f'Method/Datasets/{kb}/Test_data/Data.json', 'w') as file_test:\n",
    "#        json.dump(dict(data_test), file_test, indent=3, ensure_ascii=False)\n",
    "#        \n",
    "#    with open(f'Method/Datasets/{kb}/Train_data/Data.json', 'w') as file_train:\n",
    "#        json.dump(dict(data_train), file_train, indent=3, ensure_ascii=False)\n",
    "#        \n",
    "#    print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modern-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./Method/')\n",
    "from collections import defaultdict\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_data(data_path):\n",
    "    if not os.path.isfile(data_path):\n",
    "        with open(f\"{data_path}/Data.json\") as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        with open(f\"{data_path}\") as file:\n",
    "            data = json.load(file)\n",
    "    new_data = defaultdict(lambda: [])\n",
    "    for i,concept in tqdm(enumerate(data)):\n",
    "        positives = data[concept]['positive examples']\n",
    "        negatives = data[concept]['negative examples']\n",
    "        new_data[\"id\"].append(i)\n",
    "        new_data[\"translation\"].append({\n",
    "                         \"lang1\": \"StartPositive \" + \" \".join(positives)+\" EndPositive \"\n",
    "                         + \"StartNegative \" + \" \".join(negatives)+\" EndNegative\",\n",
    "                        \"lang2\": concept})\n",
    "    return Dataset.from_dict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electronic-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeInvalidParentheses(expression):\n",
    "    left = 0\n",
    "    right = 0\n",
    "\n",
    "    # First, we find out the number of misplaced left and right parentheses.\n",
    "    for char in expression:\n",
    "\n",
    "        # Simply record the left one.\n",
    "        if char == '(':\n",
    "            left += 1\n",
    "        elif char == ')':\n",
    "            # If we don't have a matching left, then this is a misplaced right, record it.\n",
    "            right = right + 1 if left == 0 else right\n",
    "\n",
    "            # Decrement count of left parentheses because we have found a right\n",
    "            # which CAN be a matching one for a left.\n",
    "            left = left - 1 if left > 0 else left\n",
    "\n",
    "    result = {}\n",
    "    def recurse(expression, index, left_count, right_count, left_rem, right_rem, expr):\n",
    "        # If we reached the end of the string, just check if the resulting expression is\n",
    "        # valid or not and also if we have removed the total number of left and right\n",
    "        # parentheses that we should have removed.\n",
    "        if index == len(expression):\n",
    "            if left_rem == 0 and right_rem == 0:\n",
    "                ans = \"\".join(expr)\n",
    "                result[ans] = 1\n",
    "        else:\n",
    "\n",
    "            # The discard case. Note that here we have our pruning condition.\n",
    "            # We don't recurse if the remaining count for that parenthesis is == 0.\n",
    "            if (expression[index] == '(' and left_rem > 0) or (expression[index] == ')' and right_rem > 0):\n",
    "                recurse(expression, index + 1,\n",
    "                        left_count,\n",
    "                        right_count,\n",
    "                        left_rem - int(expression[index] == '('),\n",
    "                        right_rem - int(expression[index] == ')'), expr)\n",
    "\n",
    "            expr.append(expression[index])    \n",
    "\n",
    "            # Simply recurse one step further if the current character is not a parenthesis.\n",
    "            if not expression[index] in {'(', ')'}:\n",
    "                recurse(expression, index + 1,\n",
    "                        left_count,\n",
    "                        right_count,\n",
    "                        left_rem,\n",
    "                        right_rem, expr)\n",
    "            elif expression[index] == '(':\n",
    "                # Consider an opening bracket.\n",
    "                recurse(expression, index + 1,\n",
    "                        left_count + 1,\n",
    "                        right_count,\n",
    "                        left_rem,\n",
    "                        right_rem, expr)\n",
    "            elif expression[index] == ')' and left_count > right_count:\n",
    "                # Consider a closing bracket.\n",
    "                recurse(expression, index + 1,\n",
    "                        left_count,\n",
    "                        right_count + 1,\n",
    "                        left_rem,\n",
    "                        right_rem, expr)\n",
    "\n",
    "            # Pop for backtracking.\n",
    "            expr.pop()\n",
    "\n",
    "    # Now, the left and right variables tell us the number of misplaced left and\n",
    "    # right parentheses and that greatly helps pruning the recursion.\n",
    "    recurse(expression, 0, 0, 0, left, right, [])     \n",
    "    return list(result.keys())[0]\n",
    "\n",
    "def get_predictions(kb_name='carcinogenesis', model_name='t5_small', batch_size=8):\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "    #sys.path.append('./Method/')\n",
    "    from ontolearn import KnowledgeBase\n",
    "    from owlapy.render import DLSyntaxObjectRenderer\n",
    "    \n",
    "    data = get_data(f\"Method/Datasets/{kb_name}/Test_data/\")\n",
    "    model_path = \"Method/transformers/results_\"+kb_name + \"_\" + model_name.replace(\"-\", \"_\").split(\"/\")[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/model/\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(f\"{model_path}/model/\")\n",
    "    \n",
    "    kb = KnowledgeBase(path=f\"Method/Datasets/{kb_name}/{kb_name}.owl\")\n",
    "    dl_syntax_renderer = DLSyntaxObjectRenderer()\n",
    "    atomic_concepts = list(kb.ontology().classes_in_signature())\n",
    "    atomic_concepts = [dl_syntax_renderer.render(a) for a in atomic_concepts]\n",
    "    properties = [rel.get_iri().get_remainder() for rel in kb.ontology().object_properties_in_signature()]\n",
    "    bad_tokens = list(set(tokenizer.vocab.keys())-set(['⊔', '⊓', '∃', '∀', '¬', '⊤', '⊥', ')', '(', '.'] + atomic_concepts + properties))\n",
    "    bad_tokens_ids = tokenizer(bad_tokens, add_special_tokens=False).input_ids\n",
    "                       \n",
    "    source_lang = \"lang1\"\n",
    "    target_lang = \"lang2\"\n",
    "    prefix = \"\"#\"translate instance to class expression: \"\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "        targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=64, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_data = data.map(preprocess_function, batched=True)\n",
    "    print(f'\\nTest size: {len(tokenized_data)}\\n')\n",
    "    class_expressions = []\n",
    "    for i in tqdm(range(0, len(tokenized_data), batch_size)):\n",
    "        data_batch = tokenized_data[i:i+batch_size]\n",
    "        output_sequences = model.generate(\n",
    "        input_ids=torch.tensor(data_batch['input_ids']),\n",
    "        attention_mask=torch.tensor(data_batch['attention_mask']),\n",
    "        #max_length = 12,\n",
    "        no_repeat_ngram_size = 2,\n",
    "        bad_words_ids=bad_tokens_ids,\n",
    "        do_sample=False)\n",
    "        predictions = list(map(removeInvalidParentheses, tokenizer.batch_decode(output_sequences, skip_special_tokens=True)))\n",
    "        class_expressions.extend(predictions)\n",
    "        \n",
    "    return tokenized_data, tokenizer, kb, class_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-cylinder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 23968.85it/s]\n",
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae43eae4fb84bc3b227dac150c692a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test size: 98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:42<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data, tokenizer, kb, class_expressions = get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worst-batman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Methyl ⊔ Krypton-83 ⊔ Six_ring ⊔ Sulfo   ⊔ Oxygen-41  ( ∃ hasAtom. Copper ).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeInvalidParentheses(class_expressions[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_classes.syntax_checker import SyntaxChecker, Evaluator\n",
    "#from Method.ontolearn import KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "orange-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_path = 'Method/Datasets/carcinogenesis/carcinogenesis.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustainable-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_checker = SyntaxChecker(kb, tokenizer)\n",
    "evaluator = Evaluator(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "careful-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = 'Iodine ⊔ ∃inBond.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hungry-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = {'Alcohol ⊔ (Krypton-83 ⊓ Sulfide) ⊔ Sulfo ⊔ ( ∃ hasAtom. Copper . ) ': \n",
    "              ['Alcohol','⊔','(','Krypton-83','⊓','Sulfide',')','⊔','Sulfo','⊔','∃','hasAtom','.','Copper'], \n",
    "             'Alcohol ⊓ ∀': ['Alcohol'], \n",
    "             'Iodine ⊔ ∃inBond.': ['Iodine', '⊔', '∃', 'inBond', '.', '⊤'],\n",
    "              '⊔': ['⊤'],\n",
    "              'Alcohol ⊓ ∀∃inBond.': ['Alcohol', '⊓', '∀', 'inBond', '.', '⊤'],\n",
    "              'Ar_halide ⊔ Oxygen-50 ⊔ Krypton-83 ⊔ Sulfo   ⊔ Non_ar_5c_ring .   ∃ inBond. Copper': \n",
    "              ['Ar_halide','⊔','Oxygen-50','⊔','Krypton-83','⊔','Sulfo','⊔','Non_ar_5c_ring','⊔','∃','inBond','.','Copper']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "valid-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_test(func, test_cases):\n",
    "    for key, value in test_cases.items():\n",
    "        assert func(key) == value, f'Test failed for {key}:{value}'\n",
    "    print('All test cases passed!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "enabling-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!!!\n"
     ]
    }
   ],
   "source": [
    "pass_test(syntax_checker.correct, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "trained-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 10\n",
    "ce = syntax_checker.correct(class_expressions[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tough-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  0\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  1\n",
      "Accuracy: 5.556%\n",
      "F1 score: 10.526%\n",
      "id:  2\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  3\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  4\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  5\n",
      "Accuracy: 1.2269999999999999%\n",
      "F1 score: 2.424%\n",
      "id:  6\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  7\n",
      "Accuracy: 1.5270000000000001%\n",
      "F1 score: 3.008%\n",
      "id:  8\n",
      "Accuracy: 92.553%\n",
      "F1 score: 96.133%\n",
      "id:  9\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  10\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  11\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  12\n",
      "Accuracy: 0.784%\n",
      "F1 score: 1.557%\n",
      "id:  13\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  14\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  15\n",
      "Accuracy: 17.179%\n",
      "F1 score: 29.321%\n",
      "id:  16\n",
      "Accuracy: 1.3419999999999999%\n",
      "F1 score: 2.649%\n",
      "id:  17\n",
      "Accuracy: 65.259%\n",
      "F1 score: 78.97800000000001%\n",
      "id:  18\n",
      "Accuracy: 3.857%\n",
      "F1 score: 7.428%\n",
      "id:  19\n",
      "Accuracy: 2.174%\n",
      "F1 score: 4.255%\n",
      "id:  20\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  21\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  22\n",
      "Accuracy: 0.519%\n",
      "F1 score: 1.034%\n",
      "id:  23\n",
      "Accuracy: 14.350999999999999%\n",
      "F1 score: 25.1%\n",
      "id:  24\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  25\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  26\n",
      "Accuracy: 50.955%\n",
      "F1 score: 67.511%\n",
      "id:  27\n",
      "Accuracy: 24.897%\n",
      "F1 score: 39.867999999999995%\n",
      "id:  28\n",
      "Accuracy: 3.81%\n",
      "F1 score: 7.3389999999999995%\n",
      "id:  29\n",
      "Accuracy: 24.074%\n",
      "F1 score: 38.806000000000004%\n",
      "id:  30\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  31\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  32\n",
      "Accuracy: 1.102%\n",
      "F1 score: 2.18%\n",
      "id:  33\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  34\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  35\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  36\n",
      "Accuracy: 23.358999999999998%\n",
      "F1 score: 37.871%\n",
      "id:  37\n",
      "Accuracy: 13.158%\n",
      "F1 score: 23.256%\n",
      "id:  38\n",
      "Accuracy: 7.669%\n",
      "F1 score: 14.245%\n",
      "id:  39\n",
      "Accuracy: 3.125%\n",
      "F1 score: 6.061%\n",
      "id:  40\n",
      "Accuracy: 43.333%\n",
      "F1 score: 60.465%\n",
      "id:  41\n",
      "Accuracy: 1.465%\n",
      "F1 score: 2.888%\n",
      "id:  42\n",
      "Accuracy: 3.7379999999999995%\n",
      "F1 score: 7.207%\n",
      "id:  43\n",
      "Accuracy: 65.263%\n",
      "F1 score: 78.981%\n",
      "id:  44\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  45\n",
      "Accuracy: 15.570999999999998%\n",
      "F1 score: 26.945999999999998%\n",
      "id:  46\n",
      "Accuracy: 1.562%\n",
      "F1 score: 3.077%\n",
      "id:  47\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  48\n",
      "Accuracy: 1.55%\n",
      "F1 score: 3.053%\n",
      "id:  49\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  50\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  51\n",
      "Accuracy: 0.349%\n",
      "F1 score: 0.696%\n",
      "id:  52\n",
      "Accuracy: 0.121%\n",
      "F1 score: 0.242%\n",
      "id:  53\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  54\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  55\n",
      "Accuracy: 2.613%\n",
      "F1 score: 5.093%\n",
      "id:  56\n",
      "Accuracy: 5.654%\n",
      "F1 score: 10.702%\n",
      "id:  57\n",
      "Accuracy: 5.827999999999999%\n",
      "F1 score: 11.013%\n",
      "id:  58\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  59\n",
      "Accuracy: 0.864%\n",
      "F1 score: 1.7129999999999999%\n",
      "id:  60\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  61\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  62\n",
      "Accuracy: 32.578%\n",
      "F1 score: 49.145%\n",
      "id:  63\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  64\n",
      "Accuracy: 3.604%\n",
      "F1 score: 6.957000000000001%\n",
      "id:  65\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  66\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  67\n",
      "Accuracy: 1.201%\n",
      "F1 score: 2.374%\n",
      "id:  68\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  69\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  70\n",
      "Accuracy: 13.933000000000002%\n",
      "F1 score: 24.458%\n",
      "id:  71\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  72\n",
      "Accuracy: 34.184%\n",
      "F1 score: 50.951%\n",
      "id:  73\n",
      "Accuracy: 0.5559999999999999%\n",
      "F1 score: 1.105%\n",
      "id:  74\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  75\n",
      "Accuracy: 3.125%\n",
      "F1 score: 6.061%\n",
      "id:  76\n",
      "Accuracy: 2.341%\n",
      "F1 score: 4.575%\n",
      "id:  77\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  78\n",
      "Accuracy: 2.0%\n",
      "F1 score: 3.9219999999999997%\n",
      "id:  79\n",
      "Accuracy: 9.112%\n",
      "F1 score: 16.703000000000003%\n",
      "id:  80\n",
      "Accuracy: 42.592999999999996%\n",
      "F1 score: 59.74%\n",
      "id:  81\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  82\n",
      "Accuracy: 2.5%\n",
      "F1 score: 4.878%\n",
      "id:  83\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  84\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  85\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  86\n",
      "Accuracy: 3.7740000000000005%\n",
      "F1 score: 7.273000000000001%\n",
      "id:  87\n",
      "Accuracy: 58.824%\n",
      "F1 score: 74.074%\n",
      "id:  88\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  89\n",
      "Accuracy: 2.086%\n",
      "F1 score: 4.087%\n",
      "id:  90\n",
      "Accuracy: 2.759%\n",
      "F1 score: 5.369%\n",
      "id:  91\n",
      "Accuracy: 1.119%\n",
      "F1 score: 2.212%\n",
      "id:  92\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  93\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  94\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  95\n",
      "Accuracy: 6.451999999999999%\n",
      "F1 score: 12.121%\n",
      "id:  96\n",
      "Accuracy: 0.0%\n",
      "F1 score: 0.0%\n",
      "id:  97\n",
      "Accuracy: 15.995999999999999%\n",
      "F1 score: 27.581%\n"
     ]
    }
   ],
   "source": [
    "for id in range(len(class_expressions)):\n",
    "    print('id: ', id)\n",
    "    ce = syntax_checker.correct(class_expressions[id])\n",
    "    examples = tokenized_data['translation'][id]['lang1'].split()\n",
    "    pos_end = examples.index('EndPositive')\n",
    "    exact_solution = tokenized_data['translation'][id]['lang2']\n",
    "    positives = examples[1:pos_end]\n",
    "    negatives = examples[pos_end+2:-1]\n",
    "    evaluator.evaluate(ce, positives, negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-montgomery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nces",
   "language": "python",
   "name": "nces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
